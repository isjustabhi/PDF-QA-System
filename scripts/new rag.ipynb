{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9aafde3-bbcc-4db8-9c5d-f1a1af4b381e",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e31a30-ab25-4330-9be2-63185e5054f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/abhi/pdf_qa_project/data/CWE_HW_Cleaned.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV\n",
    "csv_path = \"/Users/abhi/pdf_qa_project/data/CWE_HW_List.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Select important columns\n",
    "columns_to_keep = [\n",
    "    \"Description\", \n",
    "    \"Extended Description\", \n",
    "    \"Detection Methods\", \n",
    "    \"Potential Mitigations\"\n",
    "]\n",
    "\n",
    "# Keep only selected columns and clean up\n",
    "df_cleaned = df[columns_to_keep].dropna(how=\"all\").fillna(\"N/A\")\n",
    "\n",
    "# Save cleaned CSV\n",
    "cleaned_csv_path = \"/Users/abhi/pdf_qa_project/data/CWE_HW_Cleaned.csv\"\n",
    "df_cleaned.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "cleaned_csv_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad377c53-5263-434d-a364-3408eead07b0",
   "metadata": {},
   "source": [
    "## Convert Cleaned CSV to Markdown Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a15f8df9-5d6e-4556-9d75-e56b1d3ac83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 110 markdown files created in: /Users/abhi/pdf_qa_project/data/cwe_md_chunks\n"
     ]
    }
   ],
   "source": [
    "# Path to the cleaned CSV\n",
    "csv_path = \"/Users/abhi/pdf_qa_project/data/CWE_HW_Cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create output directory for Markdown files\n",
    "output_dir = \"/Users/abhi/pdf_qa_project/data/cwe_md_chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each row and write to .md file\n",
    "for idx, row in df.iterrows():\n",
    "    md_content = \"\\n\".join([\n",
    "        f\"**{col}**: {row[col]}\" for col in df.columns\n",
    "    ])\n",
    "    filename = os.path.join(output_dir, f\"cwe_{idx + 1}.md\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(md_content)\n",
    "\n",
    "print(f\" {len(df)} markdown files created in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e63c1c-5347-4940-8e1d-28b9fa81bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package\n",
    "pip install langchain-community langchain-ollama langchain langsmith chromadb pypdf tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a2f4a3f-09f0-45fc-bc2a-757a20297393",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangsmith\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceable\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load environment variables\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m load_dotenv(dotenv_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.env\u001b[39m\u001b[38;5;124m\"\u001b[39m), override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Configure logging\u001b[39;00m\n\u001b[1;32m     19\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langsmith import traceable\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), \"..\", \".env\"), override=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "@traceable(run_type=\"llm\", metadata={\"ls_provider\": \"ollama\", \"model\": \"mistral\"})\n",
    "def create_qa_agent(md_dir, model_name=\"mistral\"):\n",
    "    persist_directory = os.path.join(os.path.dirname(__file__), \"..\", \"data\", \"chroma_md_db\")\n",
    "\n",
    "    if os.path.exists(persist_directory):\n",
    "        logging.info(\"Loading existing Chroma store...\")\n",
    "        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=OllamaEmbeddings(model=model_name))\n",
    "    else:\n",
    "        logging.info(\"Creating new Chroma store from Markdown files...\")\n",
    "        loader = DirectoryLoader(md_dir, glob=\"*.md\", loader_cls=TextLoader)\n",
    "        docs = loader.load()\n",
    "        logging.info(f\"Loaded {len(docs)} markdown files.\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        logging.info(f\"Split the documents into {len(splits)} chunks.\")\n",
    "\n",
    "        embeddings = OllamaEmbeddings(model=model_name)\n",
    "        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "        for chunk in tqdm(splits, desc=\"Embedding markdown chunks\"):\n",
    "            vectorstore.add_documents([chunk], embedding=embeddings)\n",
    "\n",
    "        logging.info(f\"Stored {len(splits)} chunks in the vectorstore.\")\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    You are a helpful AI assistant that answers questions based on the provided Markdown documentation.\n",
    "    Use only the context provided to answer the question. If you don't know the answer or\n",
    "    can't find it in the context, say so.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    llm = Ollama(model=model_name, streaming=True)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 10}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def ask_question(qa_chain, question):\n",
    "    try:\n",
    "        response = qa_chain({\"query\": question})\n",
    "        return {\n",
    "            \"answer\": response[\"result\"],\n",
    "            \"sources\": [doc.page_content for doc in response[\"source_documents\"]]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {str(e)}\")\n",
    "        return {\n",
    "            \"error\": f\"An error occurred: {str(e)}\",\n",
    "            \"answer\": None,\n",
    "            \"sources\": None\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    md_dir = os.path.join(os.path.dirname(__file__), \"..\", \"data\", \"cwe_md_chunks\")\n",
    "\n",
    "    if not os.path.exists(md_dir):\n",
    "        logging.error(f\"The directory {md_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    qa_agent = create_qa_agent(md_dir)\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\nEnter your question (or type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        result = ask_question(qa_agent, question)\n",
    "        if result.get(\"error\"):\n",
    "            logging.error(result['error'])\n",
    "        else:\n",
    "            print(f\"\\nAnswer: {result['answer']}\")\n",
    "            print(\"Sources used:\")\n",
    "            for i, source in enumerate(result['sources'], 1):\n",
    "                print(f\"Source {i}: {source[:200]}...\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e05c10-5537-4f28-bcf6-5955edaad60c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
